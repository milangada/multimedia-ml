{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting direction of motion of object in videos\n",
    "\n",
    "This notebook covers using Microsoft CNTK to detect direction of motion of an object in a video. We will use a CNN for this purpose.Please read through CNTK_103D tutuorial to understand the basics of building a CNN using CNTK. \n",
    "The dataset used is synthetically generated. Please see gen-video-dataset-with-motion.ipynb for how the data is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cntk as C\n",
    "import subprocess as sp\n",
    "\n",
    "from cntk.io import UserMinibatchSource, StreamInformation, MinibatchData, Value\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Ensure we always get the same amount of randomness\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables\n",
    "\n",
    "Define the global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data dimensions\n",
    "global frameWidth\n",
    "global frameHeight\n",
    "global numFrames\n",
    "global numChannels\n",
    "\n",
    "global input_dim_model \n",
    "global input_dim\n",
    "global num_output_classes\n",
    "\n",
    "global cx \n",
    "global cy \n",
    "global cz\n",
    "\n",
    "global plotdata\n",
    "\n",
    "FFMPEG_BIN = \"c:\\\\vsprojects\\\\ffmpeg\\\\ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to initialize global variables\n",
    "\n",
    "A helper function to initialize the global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeVariables(channels, outputs, width, height, depth):\n",
    "    global frameWidth, frameHeight, numFrames, numChannels, input_dim_model, input_dim, num_output_classes, cx, cy, plotdata\n",
    "\n",
    "    \n",
    "    frameWidth = width\n",
    "    frameHeight = height\n",
    "    numFrames = depth\n",
    "    numChannels = channels\n",
    "    \n",
    "    input_dim_model = (channels, width, height, depth)\n",
    "    \n",
    "    input_dim = channels * width * height * depth\n",
    "    num_output_classes = outputs\n",
    "    \n",
    "    cx = C.input_variable(input_dim_model)\n",
    "    cy = C.input_variable(num_output_classes)\n",
    "    \n",
    "    plotdata = {\"minibatch_num\":[], \"loss\":[], \"error\":[], \"time\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to read frames from a video\n",
    "\n",
    "This helper function reads frames from a video and returns it as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFramesFromVideo(videoFileName, numFrames, numChannels, frameWidth, frameHeight):\n",
    "    \n",
    "    command = [FFMPEG_BIN,\n",
    "            '-i', videoFileName,\n",
    "            '-f', 'image2pipe',\n",
    "            '-pix_fmt', 'rgb24',\n",
    "            '-vcodec', 'rawvideo', '-']\n",
    "    \n",
    "    pipe = sp.Popen(command, stdout = sp.PIPE, bufsize = numChannels * frameWidth * frameHeight * 8)    \n",
    "    \n",
    "    video = np.array([])\n",
    "    for i in range(0, numFrames):\n",
    "                \n",
    "        # read numChannels * frameWidth * frameHeight bytes (= 1 frame)        \n",
    "        raw_image = pipe.stdout.read(numChannels * frameWidth * frameHeight)\n",
    "    \n",
    "        # transform the byte read into a numpy array\n",
    "        # Note that the data will come back organized as height x width x channel\n",
    "        #image =  np.fromstring(raw_image, dtype='uint8')\n",
    "        image =  np.frombuffer(raw_image, dtype='uint8')\n",
    "        \n",
    "        video = np.concatenate((video, image), axis = 0)            \n",
    "    \n",
    "        # throw away the data in the pipe's buffer.\n",
    "        pipe.stdout.flush()\n",
    "        \n",
    "    pipe.kill()\n",
    "    \n",
    "    # reshape the numpy array as it is currently a 1 dimensional array\n",
    "    video = video.reshape((numFrames, frameHeight, frameWidth, numChannels))\n",
    "    \n",
    "\n",
    "    # transpose the array to organize the data by channel x width x height x numFrames\n",
    "    video = video.transpose(3, 2, 1, 0)       \n",
    "    \n",
    "    return video\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 600, 600, 1)\n"
     ]
    }
   ],
   "source": [
    "v = readFramesFromVideo(\"./train-rgb-motion-dataset/0.mp4\", 1, 3, 600, 600)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class for reading videos in batches\n",
    "\n",
    "This is not covered in the CNTK CNN tutorial. Here we define a class that inherits from \"UserMinibatchSource\". The purpose of this class is to serve as the source of data for videos (and the corresponding labels) read in batches. This class builds upon what was covered for reading images in batches in 2D_CNN.ipynb. Read through https://cntk.ai/pythondocs/Manual_How_to_create_user_minibatch_sources.html to understand more about writing classes derived from UserMinibatchSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataSource(UserMinibatchSource):\n",
    "    def __init__(self, idx_filename, width, height, depth, channels, numLabels):\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.depth = depth\n",
    "        self.channels = channels\n",
    "        self.numLabels = numLabels                \n",
    "        \n",
    "        self.featureDimensions = channels * width * height * depth\n",
    "        \n",
    "        self.fsi = StreamInformation(\"features\", 0, 'dense', np.float32, (self.featureDimensions,))\n",
    "        self.lsi = StreamInformation(\"labels\", 1, 'dense', np.float32, (self.numLabels,))\n",
    "        \n",
    "        self.data = {}\n",
    "        self.datafiles = []\n",
    "        \n",
    "        if not os.path.isfile(idx_filename):\n",
    "            print(\"Index file \" + idx_filename + \" not found\")\n",
    "            return\n",
    "        else:\n",
    "            # Read the index file\n",
    "            \n",
    "            f = open(idx_filename, \"r\")\n",
    "            \n",
    "            lines = f.readlines()\n",
    "            for x in lines:\n",
    "                x = x.strip()\n",
    "                videoFile, videoLabel = x.split('\\t', 1)\n",
    "                \n",
    "                self.datafiles.append([videoFile, int(videoLabel)])\n",
    "            f.close()\n",
    "            \n",
    "        self.next_seq_idx = 0\n",
    "        \n",
    "        # Create a matrix to represent the one-hot labels corresponding to the outputs\n",
    "        self.label_onehot = np.eye(numLabels, dtype=int)\n",
    "        \n",
    "        super(VideoDataSource, self).__init__()\n",
    "\n",
    "    def stream_infos(self):\n",
    "        return [self.fsi, self.lsi]\n",
    "    \n",
    "    def get_checkpoint_state(self):\n",
    "        return {'next_seq_idx': self.next_seq_idx}\n",
    "    \n",
    "    def restore_from_checkpoint(self, state):\n",
    "        self.next_seq_idx = state['next_seq_idx']\n",
    "\n",
    "    def next_minibatch(self, num_samples, number_of_workers=1, worker_rank=0, device=None):\n",
    "        # Note that in this example we do not yet make use of number_of_workers or\n",
    "        # worker_rank, which will limit the minibatch source to single GPU / single node\n",
    "        # scenarios.\n",
    "\n",
    "        features = []\n",
    "        labels = []\n",
    "\n",
    "        sweep_end = False\n",
    "\n",
    "        sample_count = 0\n",
    "        \n",
    "        while (sample_count < num_samples):\n",
    "            if self.next_seq_idx == len(self.datafiles):\n",
    "                sweep_end = True\n",
    "                self.next_seq_idx = 0\n",
    "            \n",
    "            video = readFramesFromVideo(self.datafiles[self.next_seq_idx][0], self.depth, \n",
    "                                        self.channels, self.width, self.height)\n",
    "                        \n",
    "                        \n",
    "            #Assign the one-hot encoded label to l_data\n",
    "            l_data = self.label_onehot[self.datafiles[self.next_seq_idx][1]]\n",
    "            \n",
    "            features.append(video)\n",
    "            labels.append(l_data)\n",
    "                        \n",
    "            sample_count = sample_count + 1\n",
    "            self.next_seq_idx = self.next_seq_idx + 1\n",
    "            \n",
    "        num_seq = len(features)\n",
    "                               \n",
    "        f_data = Value(batch=np.asarray(features, dtype=np.float32))\n",
    "        l_data = Value(batch=np.asarray(labels, dtype=np.float32))    \n",
    "        \n",
    "        result = {            \n",
    "                cx: MinibatchData(f_data, num_seq, sample_count, sweep_end),\n",
    "                cy: MinibatchData(l_data, num_seq, sample_count, sweep_end)\n",
    "                }\n",
    "   \n",
    "\n",
    "        return result      \n",
    "\n",
    "    def print_checkpointState():\n",
    "        print(\"next_seq_idx=\" + str(self.next_seq_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test VideoDataSource class\n",
    "\n",
    "Lets test the class above to make sure we are getting data in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Input('Input3', [#], [1 x 300 x 300 x 2]): MinibatchData(data=Value([1 x 1 x 300 x 300 x 2], CPU), samples=1, seqs=1), Input('Input4', [#], [8]): MinibatchData(data=Value([1 x 8], CPU), samples=1, seqs=1)}\n"
     ]
    }
   ],
   "source": [
    "initializeVariables(1, 8, 300, 300, 2)\n",
    "v = VideoDataSource(\"./test-mono-motion.txt\", 300, 300, 2, 1, 8)\n",
    "r = v.next_minibatch(1)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to build the 3D CNN model\n",
    "\n",
    "The function below builds a 3D CNN model. To understand this, first read CNTK_103D tutorial, which explains how to build a 2D CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build model\n",
    "\n",
    "def create_model(features):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "            h = features\n",
    "            #h = C.layers.MaxPooling(filter_shape=(4,4,1), \n",
    "            #                        strides=(4,4,1), name=\"zero_max\")(h)                \n",
    "            h = C.layers.Convolution3D(filter_shape=(5,5,5), \n",
    "                                       num_filters=4, \n",
    "                                       strides=(3,3,3), \n",
    "                                       pad=True, name='first_conv')(h)\n",
    "            h = C.layers.MaxPooling(filter_shape=(2,2,2), \n",
    "                                    strides=(2,2,2), name=\"first_max\")(h)               \n",
    "            h = C.layers.Convolution3D(filter_shape=(5,5,5), \n",
    "                                       num_filters=4, \n",
    "                                       strides=(3,3,3), \n",
    "                                       pad=True, name='second_conv')(h)\n",
    "            h = C.layers.MaxPooling(filter_shape=(2,2,2), \n",
    "                                    strides=(2,2,2), name=\"second_max\")(h)        \n",
    "            #h = C.layers.Convolution3D(filter_shape=(3,3,3), \n",
    "            #                           num_filters=4, \n",
    "            #                           strides=(2,2,2), \n",
    "            #                           pad=True, name='third_conv')(h)\n",
    "            #h = C.layers.MaxPooling(filter_shape=(2,2,2), \n",
    "            #                        strides=(2,2,1), name=\"third_max\")(h)              \n",
    "            \n",
    "            r = C.layers.Dense(num_output_classes, activation=None, name='classify')(h)\n",
    "\n",
    "            return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More helper functions\n",
    "The functions below are copied from CNTK_103D tutorial (read that tutorial for a better understanding of what they do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criterion_function(model, labels):\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error(model, labels)\n",
    "    return loss, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, start_time, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "    time_since_start = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        time_since_start = \"{:.1f}\".format(time.time() - start_time)\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100) + \n",
    "                   \", Time since start: \" + time_since_start)\n",
    "        \n",
    "    return mb, training_loss, eval_error, time_since_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train CNN based on provided training dataset\n",
    "The function below goes through the cycle of training the CNN. There is mechanism to create a checkpoint for every minibatch. Read through https://cntk.ai/pythondocs/Manual_How_to_train_using_declarative_and_imperative_API.html to learn more about creating checkpoints to avoid restarting training from scratch in the event of an unexpected failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_train(train_reader, model_func, logFile, checkpointFile, \n",
    "                train_minibatch_size = 30, num_samples_per_sweep = 40000, \n",
    "                num_sweeps_to_train_with = 10, learning_rate = 0.1, training_progress_output_freq = 1):\n",
    "    \n",
    "    global plotdata\n",
    "    \n",
    "    print(\"Training minibatch size = \" + str(train_minibatch_size))\n",
    "    print(\"Number of samples per sweep = \" + str(num_samples_per_sweep))\n",
    "    print(\"Number of sweeps to train with = \" + str(num_sweeps_to_train_with))\n",
    "    print(\"Learning rate = \" + str(learning_rate))\n",
    "    print(\"Number of frames per video = \" + str(numFrames))\n",
    "    \n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    # We will scale the input image pixels within 0-1 range by dividing all input value by 255.\n",
    "    model = model_func(cx/255)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(model, cy)\n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(cz.parameters, lr_schedule)\n",
    "    trainer = C.Trainer(cz, (loss, label_error), [learner])\n",
    "    \n",
    "    # Initialize the parameters for the trainer\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / train_minibatch_size\n",
    "           \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "    \n",
    "    print(\"Number of mini batches to train = \" + str(num_minibatches_to_train))\n",
    "    \n",
    "    fLogFile = open(logFile, \"a+\")\n",
    "\n",
    "    if os.path.exists(checkpointFile):\n",
    "        print(\"Trying to restore from checkpoint\")\n",
    "        #print(train_reader.next_seq_idx)\n",
    "        \n",
    "        mb_source_state = trainer.restore_from_checkpoint(checkpointFile)  \n",
    "        train_reader.restore_from_checkpoint(mb_source_state)\n",
    "\n",
    "        print(\"Restore has finished successfully\")\n",
    "        #print(train_reader.next_seq_idx)\n",
    "    \n",
    "    else:\n",
    "        print(\"No restore file found\")    \n",
    "    \n",
    "    train = True\n",
    "    iteration_num = int(trainer.total_number_of_samples_seen / train_minibatch_size)\n",
    "    while (train and (iteration_num < num_minibatches_to_train)):\n",
    "    #for i in range(0, int(num_minibatches_to_train)):\n",
    "            \n",
    "        # Read a mini batch from the training dataset\n",
    "        data=train_reader.next_minibatch(train_minibatch_size)\n",
    "        \n",
    "        train = trainer.train_minibatch(data)\n",
    "        mb, training_loss, eval_error, time_since_start = print_training_progress(trainer, iteration_num, \n",
    "                                                                                  training_progress_output_freq, start, \n",
    "                                                                                  verbose=0)\n",
    "        \n",
    "        mb_source_state = train_reader.get_checkpoint_state()\n",
    "        trainer.save_checkpoint(checkpointFile, mb_source_state)\n",
    "        \n",
    "        \n",
    "        if ((iteration_num % training_progress_output_freq) == 0):            \n",
    "            logLine = \"{0}\\t{1:.4f}\\t{2:.2f}%\\t{3}\\t{4}\\n\".format(mb, training_loss, eval_error*100, time_since_start,\n",
    "                                                                 datetime.now().strftime('%H:%M:%S'))\n",
    "            fLogFile.write(logLine)      \n",
    "            fLogFile.flush()                  \n",
    "     \n",
    "        iteration_num = int(trainer.total_number_of_samples_seen / train_minibatch_size)\n",
    "        \n",
    "    # Print training time\n",
    "    print(\"Training took {:.1f} sec\".format(time.time() - start))\n",
    "    \n",
    "    fLogFile.close()\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test the trained CNN\n",
    "The function below goes through the cycle of testing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_test(trainer, test_reader, num_test_samples = 8000, test_minibatch_size = 50, print_frequency = 500):\n",
    "    \n",
    "    # Test data for trained model       \n",
    "    num_minibatches_to_test = num_test_samples // test_minibatch_size\n",
    "\n",
    "    print(\"Number of minibatches to test = \" + str(num_minibatches_to_test))\n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(0, num_minibatches_to_test):               \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        data = test_reader.next_minibatch(test_minibatch_size)\n",
    "            \n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "        \n",
    "        if ((i % print_frequency) == 0):\n",
    "            print(\"Testing minibatch \" + str(i) + \" Eval error = \" + str(eval_error))\n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    print(\"Average test error: {0:.2f}%\".format(test_result * 100 / num_minibatches_to_test))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create readers and trigger training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(train_file, trainLogFile, test_file, modelFile, train_minibatch_size = 30, num_samples_per_sweep = 40000, \n",
    "                   num_sweeps_to_train_with = 2, learning_rate = 0.2):\n",
    "    global cz\n",
    "    \n",
    "    cz = create_model(cx)\n",
    "    reader_train = VideoDataSource(train_file, frameWidth, frameHeight, numFrames, numChannels, num_output_classes)    \n",
    "    reader_test = VideoDataSource(test_file, frameWidth, frameHeight, numFrames, numChannels, num_output_classes)\n",
    "    \n",
    "    trainer = video_train(reader_train, cz, trainLogFile, modelFile, train_minibatch_size, \n",
    "                          num_samples_per_sweep, num_sweeps_to_train_with, learning_rate)\n",
    "    video_test(trainer, reader_test)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot charts\n",
    "Function to plot the charts to showcase progress during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCharts():\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.plot(plotdata[\"minibatch_num\"], plotdata[\"loss\"], 'b--')\n",
    "    plt.xlabel('Minibatch number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Minibatch run vs. Training loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(plotdata[\"minibatch_num\"], plotdata[\"error\"], 'r--')\n",
    "    plt.xlabel('Minibatch number')\n",
    "    plt.ylabel('Label Prediction Error')\n",
    "    plt.title('Minibatch run vs. Label Prediction Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for black and white videos\n",
    "Finally, use all the functions above to initialize variables, create the model and start the training and testing cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 6612 parameters in 6 parameter tensors.\n",
      "Training minibatch size = 30\n",
      "Number of samples per sweep = 40000\n",
      "Number of sweeps to train with = 2\n",
      "Learning rate = 0.2\n",
      "Number of frames per video = 90\n",
      "Number of mini batches to train = 2666.6666666666665\n",
      "Trying to restore from checkpoint\n",
      "Restore has finished successfully\n"
     ]
    }
   ],
   "source": [
    "initializeVariables(1, 8, 300, 300, 90)\n",
    "\n",
    "trainIdx = \"./train-mono-motion.txt\"\n",
    "testIdx = \"./test-mono-motion.txt\"\n",
    "trainLogFile = \"./train-mono-motion-log.txt\"\n",
    "checkpointFile = \"./chk-mono-8motions-video.dnn\"\n",
    "modelFile = \"./mono-8motions-video.dnn\"\n",
    "\n",
    "# Create the model\n",
    "cz = create_model(cx)\n",
    "\n",
    "# Number of parameters in the network\n",
    "C.logging.log_number_of_parameters(cz)\n",
    "\n",
    "train_and_test(trainIdx, trainLogFile, testIdx, checkpointFile)\n",
    "\n",
    "if os.path.isfile(modelFile):\n",
    "    os.remove(modelFile)\n",
    "\n",
    "# Save the model to a file\n",
    "cz.save(modelFile)\n",
    "\n",
    "#plotCharts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C.device.all_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertVideosToNumpyArrays(videoFilesIdx, videoNumpyArraysIdx, videoNumpyFilesDir,\n",
    "                               numFrames, numChannels, frameWidth, frameHeight):\n",
    "\n",
    "    if not os.path.isfile(videoFilesIdx):\n",
    "        print(\"Video index file \" + videoFilesIdx + \" not found\")\n",
    "        return\n",
    "    else:\n",
    "        # Read the videos from index file and write them out as numpy array files\n",
    "        f = open(videoFilesIdx, \"r\")\n",
    "        fvn = open(videoNumpyArraysIdx, \"w\")\n",
    "\n",
    "        lines = f.readlines()\n",
    "        for x in lines:\n",
    "            x = x.strip()\n",
    "            videoFile, videoLabel = x.split('\\t', 1)\n",
    "            \n",
    "            videoNumpyArrayFileName = videoNumpyFilesDir + videoFile.split('/')[-1] + \".npy\"            \n",
    "            videoNumpyArray = readFramesFromVideo(videoFile, numFrames, numChannels, frameWidth, frameHeight)\n",
    "            np.save(videoNumpyArrayFileName, videoNumpyArray)\n",
    "            \n",
    "            fvn.write(videoNumpyArrayFileName + \"/t\" + videoLabel)\n",
    "            \n",
    "        f.close()\n",
    "        fvn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertVideosToNumpyArrays(\"./train-bw-motion.txt\", \"./train-bw-npy-motion.txt\", \"./train-bw-npy-motion-dataset/\",\n",
    "                           45, 1, 600, 600)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
